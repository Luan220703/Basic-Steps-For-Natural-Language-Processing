{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stop')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. How to tokenize a given text?\n",
    "# Tokeniation with nltk\n",
    "text = \" I wat gay\"\n",
    "tokens=nltk.word_tokenize(text)\n",
    "for token in tokens:\n",
    "  print(token)\n",
    "# Tokenization with spaCy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(text)\n",
    "for token in doc:\n",
    "  print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopword by NLTK\n",
    "from nltk.corpus import stopwords\n",
    "my_stopwords=set(stopwords.words('english'))\n",
    "new_tokens=[]\n",
    "\n",
    "# Tokenization using word_tokenize()\n",
    "all_tokens=nltk.word_tokenize(text)\n",
    "\n",
    "for token in all_tokens:\n",
    "  if token not in my_stopwords:\n",
    "    new_tokens.append(token)\n",
    "    \n",
    "\" \".join(new_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to do spell correction in a given text ?\n",
    "# Import textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Using textblob's correct() function\n",
    "text=TextBlob(text)\n",
    "print(text.correct())\n",
    "#> He is a great person. He believes in god"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to extract all the nouns in a text?\n",
    "# Coverting the text into a spacy Doc\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(text)\n",
    "\n",
    "# Using spacy's pos_ attribute to check for part of speech tags\n",
    "for token in doc:\n",
    "  if token.pos_=='NOUN' or token.pos_=='PROPN':\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to extract all the pronouns in a text?\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "  if token.pos_=='PRON':\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to find cosine similary with matrix\n",
    "# Using Vectorizer of sklearn to get vector representation\n",
    "text1='Taj Mahal is a tourist place in India'\n",
    "text2='Great Wall of China is a tourist place in china'\n",
    "documents=[text1,text2]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer=CountVectorizer()\n",
    "matrix=vectorizer.fit_transform(documents)\n",
    "\n",
    "# Obtaining the document-word matrix\n",
    "doc_term_matrix=matrix.todense()\n",
    "doc_term_matrix\n",
    "\n",
    "# Computing cosine similarity\n",
    "df=pd.DataFrame(doc_term_matrix)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df,df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "model=Word2Vec()\n",
    "\n",
    "sentence_orange = 'Oranges are my favorite fruit'\n",
    "sent=\"apples are not my favorite\"\n",
    "\n",
    "# Computing the word mover distance\n",
    "distance = model.wmdistance(sent, sentence_orange)\n",
    "\n",
    "#> 5.378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to extract topic keywords using LSA?\n",
    "# Importing the Tf-idf vectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Defining the vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000,  max_df = 0.5, smooth_idf=True)\n",
    "\n",
    "# Transforming the tokens into the matrix form through .fit_transform()\n",
    "matrix= vectorizer.fit_transform(texts)\n",
    "\n",
    "# SVD represent documents and terms in vectors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "SVD_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=100, random_state=122)\n",
    "SVD_model.fit(matrix)\n",
    "\n",
    "# Getting the terms \n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# Iterating through each topic\n",
    "for i, comp in enumerate(SVD_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    # sorting the 7 most important terms\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    # printing the terms of a topic\n",
    "    for t in sorted_terms:\n",
    "        print(t[0],end=' ')\n",
    "    print(' ')\n",
    "\n",
    "#> Topic 0: \n",
    "#> learn new life travelling country feel  \n",
    "#> Topic 1: \n",
    "#> life cherish diaries let share experience  \n",
    "#> Topic 2: \n",
    "#> feel know time people just regions  \n",
    "#> Topic 3: \n",
    "#> time especially cherish diaries let share  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sentiment analysis with TextBlob\n",
    "from textblob import TextBlob\n",
    "blob=TextBlob(text)\n",
    "\n",
    "# Using the sentiment attribute \n",
    "print(blob.sentiment)\n",
    "if(blob.sentiment.polarity > 0):\n",
    "  print(\"Positive\")\n",
    "\n",
    "#> Sentiment(polarity=0.9533333333333333, subjectivity=1.0)\n",
    "#> Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bigrams and trigrams\n",
    "from nltk import ngrams\n",
    "bigram=list(ngrams(Sentences.lower().split(),2))\n",
    "trigram=list(ngrams(Sentences.lower().split(),3))\n",
    "\n",
    "print(\" Bigrams are\",bigram)\n",
    "print(\" Trigrams are\", trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to detect the language of entered text ?\n",
    "# Install spacy's languagedetect library\n",
    "import spacy\n",
    "!pip install spacy_langdetect\n",
    "from spacy_langdetect import LanguageDetector\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Add the language detector to the processing pipeline\n",
    "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "doc = nlp(text)\n",
    "# document level language detection. Think of it like average language of the document!\n",
    "print(doc._.language)\n",
    "# sentence level language detection\n",
    "for sent in doc.sents:\n",
    "   print(sent, sent._.language)\n",
    "\n",
    "#> {'language': 'es', 'score': 0.9999963653206719}\n",
    "#> El agente imprime su pase de abordaje. {'language': 'es', 'score': 0.9999969081229643}\n",
    "#> Los oficiales de seguridad del aeropuerto pasan junto a él con un perro grande. {'language': 'es', 'score': 0.9999951631258189}\n",
    "#> El perro está olfateando alrededor del equipaje de las personas tratando de detectar drogas o explosivos. {'language': 'es', 'score': 0.9999938903880353}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to extract first name and last names present in the document ?\n",
    "text=\"Sherlock Holmes and Clint Thomas were good friends. I am a fan of John Mark\"\n",
    "# Import and initialize spacy's matcher\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "doc=nlp(text)\n",
    "\n",
    "# Function that adds patterns to the matcher and finds the respective matches\n",
    "def extract_matches(doc):\n",
    "   pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "   matcher.add('FULL_NAME', None, pattern)\n",
    "   matches = matcher(doc)\n",
    "   \n",
    "   for match_id, start, end in matches:\n",
    "     span = doc[start:end]\n",
    "     print(span.text)\n",
    "\n",
    "extract_matches(doc)\n",
    "\n",
    "\n",
    "#> Sherlock Holmes\n",
    "#> Clint Thomas\n",
    "#> John Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to identify named entities in the given text\n",
    "text=\" Walter works at Google. He lives in London.\"\n",
    "# Load spacy modelimport spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(text)\n",
    "# Using the ents attribute of doc, identify labels\n",
    "for entity in doc.ents:  \n",
    "   print(entity.text,entity.label_)\n",
    "\n",
    "#> Walter PERSON\n",
    "#> Google ORG\n",
    "#> London GPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to identify all the names of Organizations present in the text with NER ?\n",
    "doc=nlp(text)\n",
    "list_of_org=[]\n",
    "for entity in doc.ents:\n",
    "  if entity.label_==\"ORG\":\n",
    "    list_of_org.append(entity.text)\n",
    "\n",
    "print(list_of_org)\n",
    "#> ['Google', 'Amazon', 'Apple', 'Flipkart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to replace all names of people in the text with ‘UNKNOWN’\n",
    "\n",
    "doc=nlp(news)\n",
    "\n",
    "# Identifying the entities of category 'PERSON'\n",
    "entities = [entity.text  for entity in doc.ents  if entity.label_=='PERSON']\n",
    "updated_text=[]\n",
    "\n",
    "for token in doc:\n",
    "  if token.text in entities:\n",
    "    updated_text.append(\"UNKNOWN\")\n",
    "  else :\n",
    "    updated_text.append(token.text)\n",
    "\n",
    "\" \".join(updated_text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to visualize the named entities using spaCy\n",
    "# Use spacy's displacy with the parameter style=\"ent\"\n",
    "from spacy import displacy\n",
    "doc=nlp(text)\n",
    "displacy.render(doc,style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to implement dependency parsing ?\n",
    "# Using dep_ attribute od tokens in spaCy to access the dependency of the word in sentence.\n",
    "doc=nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "  print(token.text,token.dep_)\n",
    "\n",
    "\n",
    "#> Mark nsubj\n",
    "#> plays ROOT\n",
    "#> volleyball dobj\n",
    "#> every det\n",
    "#> evening npadvmod\n",
    "#> . punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to find the ROOT word of any word in a sentence?\n",
    "# use the head attribute of tokens to find it's rootword\n",
    "text=\"Mark plays volleyball. Sam is not into sports, he paints a lot\"\n",
    "doc=nlp(text)\n",
    "for token in doc:\n",
    "  print(token.text,token.head)\n",
    "\n",
    "#> Mark plays\n",
    "#> plays plays\n",
    "#> volleyball plays\n",
    "#> . plays\n",
    "#> Sam is\n",
    "#> is paints\n",
    "#> not is\n",
    "#> into is\n",
    "#> sports into\n",
    "#> , paints\n",
    "#> he paints\n",
    "#> paints paints\n",
    "#> a lot\n",
    "#> lot paints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to build a text classifier with TextBlob ?\n",
    "# Importing the classifier\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Training\n",
    "cl = NaiveBayesClassifier(train)\n",
    "\n",
    "# Classify some text\n",
    "print(cl.classify(\"My favorite food is spring rolls\"))  \n",
    "print(cl.classify(\"It was a cold place for picnic\"))  \n",
    "\n",
    "# Printing accuracy of classifier\n",
    "print(\"Accuracy: {0}\".format(cl.accuracy(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Sample text\n",
    "text = \"He would also attend the opening ceremony for the construction of the US. Embassy complex in Cau Giay District, as well as meeting students, teachers and scientists at the Hanoi University of Science and Technology\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Filter words with less than 4 letters\n",
    "filtered_words = [word.lower() for word in words if len(word) >= 4]\n",
    "\n",
    "# Calculate frequency distribution\n",
    "fdist = FreqDist(filtered_words)\n",
    "\n",
    "# Sort the words by frequency\n",
    "sorted_words = sorted(fdist.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the words in decreasing order of frequency\n",
    "word_frequency_list = [(word, frequency) for word, frequency in sorted_words]\n",
    "print(word_frequency_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
